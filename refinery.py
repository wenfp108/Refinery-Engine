import os, json, base64, requests, importlib.util, sys
from datetime import datetime, timedelta, timezone
import pandas as pd
from supabase import create_client
from github import Github, Auth

# === ğŸ›¡ï¸ 1. æ ¸å¿ƒé…ç½® ===
PRIVATE_BANK_ID = "wenfp108/Central-Bank" 
GITHUB_TOKEN = os.environ.get("GH_PAT") 
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not all([GITHUB_TOKEN, SUPABASE_URL, SUPABASE_KEY]):
    sys.exit("âŒ [å®¡è®¡å¼‚å¸¸] ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
auth = Auth.Token(GITHUB_TOKEN)
gh_client = Github(auth=auth)
private_repo = gh_client.get_repo(PRIVATE_BANK_ID)

# === ğŸ§© 2. æ’ä»¶å‘ç°ç³»ç»Ÿ ===
def get_all_processors():
    procs = {}
    proc_dir = "./processors"
    if not os.path.exists(proc_dir): return procs
    for filename in os.listdir(proc_dir):
        if filename.endswith(".py") and not filename.startswith("__"):
            name = filename[:-3]
            try:
                spec = importlib.util.spec_from_file_location(f"mod_{name}", os.path.join(proc_dir, filename))
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                procs[name] = {
                    "module": mod,
                    "table_name": getattr(mod, "TABLE_NAME", f"{name}_logs"),
                }
            except Exception as e: print(f"âš ï¸ æ’ä»¶ {name} åŠ è½½å¤±è´¥: {e}")
    return procs

# === â±ï¸ è¾…åŠ©ï¼šæ£€æŸ¥æ•°æ®æ–°é²œåº¦ ===
def get_data_freshness(table_name):
    try:
        res = supabase.table(table_name).select("bj_time").order("bj_time", desc=True).limit(1).execute()
        if not res.data: return (False, 9999, "æ— æ•°æ®")
        
        last_time_str = res.data[0]['bj_time']
        if not last_time_str: return (False, 9999, "æ— æ—¶é—´æˆ³")

        try:
            last_time_str = last_time_str.replace('Z', '+00:00')
            last_time = datetime.fromisoformat(last_time_str)
        except:
            return (False, 9999, last_time_str)
        
        now = datetime.now(timezone(timedelta(hours=8)))
        if last_time.tzinfo is None:
            last_time = last_time.replace(tzinfo=timezone(timedelta(hours=8)))
        
        diff = now - last_time
        minutes_ago = int(diff.total_seconds() / 60)
        
        return (minutes_ago <= 65, minutes_ago, last_time.strftime('%H:%M'))
    except Exception as e:
        print(f"âš ï¸ æ–°é²œåº¦æ£€æŸ¥å¤±è´¥ ({table_name}): {e}")
        return (True, 0, "CheckError")

# === ğŸ”¥ 3. æˆ˜æŠ¥å·¥å‚ï¼šMarkdown å‚ç›´å †å å¼•æ“ ===

def generate_hot_reports(processors_config):
    print("\nğŸ”¥ [æƒ…æŠ¥å¯¹å†²] æ­£åœ¨ç”Ÿæˆ Markdown æ—¶æŠ¥...")
    bj_now = datetime.now(timezone(timedelta(hours=8)))
    
    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 1] è‡ªå®šä¹‰æ–‡ä»¶åæ ¼å¼: 2026-02-01-14.md
    # ä½¿ç”¨ä¸­åˆ’çº¿åˆ†éš”ï¼Œæ–¹ä¾¿é˜…è¯»
    file_name = bj_now.strftime('%Y-%m-%d-%H') + ".md"
    report_path = f"reports/{file_name}"
    
    date_display = bj_now.strftime('%Y-%m-%d %H:%M')
    
    md_report = f"# ğŸš€ Architect's Alpha æƒ…æŠ¥å®¡è®¡ ({date_display})\n\n"
    md_report += "> **æœºåˆ¶è¯´æ˜**ï¼šå…¨æºæ™ºèƒ½å»é‡ | æ— æ›´æ–°æºè‡ªåŠ¨æŠ˜å \n\n"

    has_content = False

    for source_name, config in processors_config.items():
        if hasattr(config["module"], "get_hot_items"):
            try:
                table = config["table_name"]
                is_fresh, mins_ago, last_update_time = get_data_freshness(table)
                
                # ç¡çœ æ¨¡å¼
                if not is_fresh:
                    md_report += f"## ğŸ’¤ æ¥æºï¼š{source_name.upper()} (ä¸Šæ¬¡æ›´æ–°: {last_update_time})\n"
                    md_report += f"> *è·ä¸Šæ¬¡æ›´æ–°å·²è¿‡ {int(mins_ago/60)} å°æ—¶ï¼Œæš‚æ— æ–°æ•°æ®ã€‚*\n\n"
                    continue 

                # å”¤é†’æ¨¡å¼
                sector_matrix = config["module"].get_hot_items(supabase, table)
                if not sector_matrix: continue

                has_content = True
                md_report += f"## ğŸ“¡ æ¥æºï¼š{source_name.upper()}\n"
                
                for sector, items in sector_matrix.items():
                    md_report += f"### ğŸ·ï¸ æ¿å—ï¼š{sector}\n"
                    md_report += "| ä¿¡å·å¼ºåº¦ | æºå¤´ | å…³é”®æƒ…æŠ¥æ‘˜è¦ | é“¾æ¥ |\n| :--- | :--- | :--- | :--- |\n"
                    
                    for item in items:
                        score = int(item.get('score', 0))
                        source = item.get('user_name', 'Unknown')
                        text = item.get('full_text', '').replace('\n', ' ')[:85] + "..."
                        url = item.get('url') or item.get('tweet_url') or '#'
                        
                        md_report += f"| **{score:,}** | {source} | {text} | [æŸ¥çœ‹]({url}) |\n"
                    md_report += "\n"
            except Exception as e:
                print(f"âš ï¸ {source_name} æ¸²æŸ“å¼‚å¸¸: {e}")

    if not has_content:
        md_report += "\n\n**ğŸ›‘ æœ¬è½®æ‰«æå…¨åŸŸé™é»˜ï¼Œè¯·æŸ¥é˜…å†å²å½’æ¡£ã€‚**"

    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 2] åªå†™å…¥æ‚¨æŒ‡å®šçš„è¿™ä¸€ä¸ªæ–‡ä»¶ï¼Œä¸å†å†™ latest_brief.md
    try:
        try:
            old = private_repo.get_contents(report_path)
            private_repo.update_file(old.path, f"ğŸ“Š Update: {file_name}", md_report, old.sha)
            print(f"âœ… æ›´æ–°æˆ˜æŠ¥: {report_path}")
        except:
            private_repo.create_file(report_path, f"ğŸš€ New: {file_name}", md_report)
            print(f"âœ… åˆ›å»ºæˆ˜æŠ¥: {report_path}")
    except Exception as e: print(f"âŒ å†™å…¥ {report_path} å¤±è´¥: {e}")

# === ğŸ¦ 5. æ¬è¿é€»è¾‘ (æ”¯æŒå…¨é‡è¡¥å½•) ===

def process_and_upload(path, sha, config):
    # æŸ¥é‡ï¼šå¦‚æœæ–‡ä»¶å·²å¤„ç†ï¼Œç§’é€€
    check = supabase.table("processed_files").select("file_sha").eq("file_sha", sha).execute()
    if check.data: return False 
    
    print(f"ğŸ“¥ æ­£åœ¨å¤„ç†: {path} ...")
    try:
        content_file = private_repo.get_contents(path)
        raw_data = json.loads(base64.b64decode(content_file.content).decode('utf-8'))
        
        items = config["module"].process(raw_data, path)
        if items:
            # æ‰¹é‡æ’å…¥
            for i in range(0, len(items), 500):
                supabase.table(config["table_name"]).insert(items[i : i+500]).execute()
            
            # æ ‡è®°å·²å¤„ç†
            supabase.table("processed_files").upsert({
                "file_sha": sha, 
                "file_path": path,
                "engine": config.get("table_name", "unknown").split('_')[0],
                "item_count": len(items)
            }).execute()
            return True
    except Exception as e: 
        print(f"âš ï¸ è§£æå¤±è´¥ {path}: {e}")
    return False

def sync_bank_to_sql(processors_config, full_scan=False):
    """
    åŒæ¨¡å¼åŒæ­¥ï¼š
    - full_scan=True: åœ°æ¯¯å¼æ‰«ææ•´ä¸ªä»“åº“ (é€’å½’éå†)
    - full_scan=False: åªçœ‹è¿‡å» 24h æäº¤ (å¿«)
    """
    if full_scan:
        # ğŸ”¥ [ä¿®æ”¹ç‚¹ 3] çœŸæ­£çš„é€’å½’å…¨é‡æ‰«æé€»è¾‘
        print("\nğŸšœ [å…¨é‡æ¨¡å¼] æ­£åœ¨åœ°æ¯¯å¼æ‰«æ Central-Bank æ‰€æœ‰å†å²æ–‡ä»¶...")
        try:
            contents = private_repo.get_contents("")
            while contents:
                file_content = contents.pop(0)
                if file_content.type == "dir":
                    contents.extend(private_repo.get_contents(file_content.path))
                elif file_content.name.endswith(".json"):
                    # æ‰¾åˆ° JSONï¼Œåˆ¤æ–­å±äºå“ªä¸ªæ’ä»¶
                    source_key = file_content.path.split('/')[0] # twitter, polymarket...
                    if source_key in processors_config:
                        process_and_upload(file_content.path, file_content.sha, processors_config[source_key])
        except Exception as e:
            print(f"âŒ å…¨é‡æ‰«æä¸­æ–­: {e}")
            
    else:
        print("\nâš¡ [å¢é‡æ¨¡å¼] æ­£åœ¨æ£€æŸ¥è¿‡å» 24h çš„æäº¤...")
        since = datetime.now(timezone.utc) - timedelta(hours=24)
        commits = private_repo.get_commits(since=since)
        for commit in commits:
            for f in commit.files:
                if f.filename.endswith('.json'):
                    source_key = f.filename.split('/')[0]
                    if source_key in processors_config:
                        process_and_upload(f.filename, f.sha, processors_config[source_key])

# === ğŸš€ 6. æ‰§è¡Œå…¥å£ ===
if __name__ == "__main__":
    all_procs = get_all_processors()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ FORCE_FULL_SCAN æ˜¯å¦ä¸º true
    is_full_scan = (os.environ.get("FORCE_FULL_SCAN") == "true")
    
    # 1. åŒæ­¥æ•°æ®
    sync_bank_to_sql(all_procs, full_scan=is_full_scan)
    
    # 2. ç”Ÿæˆæˆ˜æŠ¥
    generate_hot_reports(all_procs)
    
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] âœ… å®¡è®¡ä»»åŠ¡å®Œæˆã€‚")
