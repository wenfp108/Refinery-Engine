import json
from datetime import datetime, timedelta

# === é…ç½®åŒº ===
# å¯¹åº” Supabase é‡Œçš„è¡¨å (è®°å¾—å» Supabase SQL Editor æ‰§è¡Œå»ºè¡¨è¯­å¥)
TABLE_NAME = "reddit_logs"

# ç›®æ ‡é‡‘è/ç§‘æŠ€æ¿å— (ç”¨äº"å¸‚åœºé£å‘"ç­–ç•¥ç­›é€‰)
TARGET_MARKET_SUBS = [
    'wallstreetbets', 'stocks', 'economy', 'options', 'bitcoin', 
    'technology', 'hardware', 'semiconductors', 'futurology', 'investing'
]

# === 0. è¾…åŠ©å·¥å…· ===
def fmt_k(num):
    """ å°†æ•°å­—æ ¼å¼åŒ–ä¸º K/M (e.g. 1.2K, 15M) """
    if not num: return "0"
    try: n = float(num)
    except: return "0"
    if n >= 1_000_000: return f"{n/1_000_000:.1f}M"
    if n >= 1_000: return f"{n/1_000:.1f}K"
    return str(int(n))

# === 1. æ•°æ®æ¸…æ´—é€»è¾‘ (ETL) ===
# è´Ÿè´£è§£æ sentiment/ ç›®å½•ä¸‹é‚£ç§åµŒå¥—çš„ JSON ç»“æ„ï¼Œå¹¶å±•å¹³ä¸ºæ•°æ®åº“è¡Œ
def process(raw_data, path):
    # å…¼å®¹å¤„ç†ï¼šå¦‚æœå¤–å±‚æ˜¯åˆ—è¡¨ï¼ˆæ ‡å‡†ç»“æ„ï¼‰ï¼Œå–åˆ—è¡¨ï¼›å¦‚æœæ˜¯å­—å…¸ï¼ŒåŒ…ä¸€å±‚
    items = raw_data if isinstance(raw_data, list) else [raw_data]
    
    refined_results = []
    
    for batch in items:
        # 1. æå–æ‰¹æ¬¡æ—¶é—´ (JSON é‡Œçš„ timestamp)
        # æ ¼å¼ç¤ºä¾‹: "2026-02-05T01:23:38.695680+08:00"
        ts = batch.get('timestamp')
        if not ts: ts = datetime.now().isoformat()
        
        # 2. éå†æ¿å— (data åˆ—è¡¨)
        for sub_data in batch.get('data', []):
            subreddit = sub_data.get('subreddit')
            
            # 3. éå†å† å†›å¸–å­ (champions åˆ—è¡¨)
            for post in sub_data.get('champions', []):
                # æ„é€ æ•°æ®åº“è¡Œç»“æ„
                row = {
                    "bj_time": ts,
                    "subreddit": subreddit,
                    "title": post.get('title'),
                    "url": post.get('url'),
                    "summary": post.get('summary'),
                    "score": int(post.get('score', 0)),
                    "vibe": float(post.get('vibe', 0.0)),
                    "raw_json": post  # å¤‡ä»½åŸå§‹æ•°æ®ä»¥å¤‡åç”¨
                }
                refined_results.append(row)
                
    return refined_results

# === 2. æˆ˜æŠ¥ç”Ÿæˆé€»è¾‘ (åˆ†ç±»ç‹¬ç«‹ç‰ˆ) ===
def get_hot_items(supabase, table_name):
    # A. è·å–æœ€è¿‘ 24 å°æ—¶çš„æ•°æ®
    yesterday = (datetime.now() - timedelta(hours=24)).isoformat()
    try:
        # æŸ¥åº“ï¼šæŒ‰æ—¶é—´å€’åº
        res = supabase.table(table_name).select("*").gt("bj_time", yesterday).execute()
        all_posts = res.data if res.data else []
    except Exception as e:
        print(f"Reddit DB Error: {e}")
        return {}
    
    if not all_posts: return {}

    # B. å»é‡é€»è¾‘ (Deduplication)
    # åŒä¸€ä¸ª URL å¯èƒ½åœ¨ä¸åŒæ—¶é—´ç‚¹è¢«æŠ“å–å¤šæ¬¡ï¼Œæˆ‘ä»¬åªä¿ç•™æ—¶é—´æˆ³æœ€æ–°çš„é‚£ä¸ª
    unique_map = {}
    for p in all_posts:
        url = p.get('url')
        if not url: continue  # ä¸¢å¼ƒæ²¡æœ‰ URL çš„è„æ•°æ®
        
        # å¦‚æœ URL å·²å­˜åœ¨ï¼Œä¸”å½“å‰è¿™æ¡çš„æ—¶é—´æ›´æ–°ï¼Œåˆ™è¦†ç›–
        if url not in unique_map or p['bj_time'] > unique_map[url]['bj_time']:
            unique_map[url] = p
            
    posts = list(unique_map.values())

    # C. åˆ†ç±»ç­›é€‰å™¨ (The Filter Pipeline)
    
    # --- ç­–ç•¥ 1: ğŸš¨ å…¨çƒç»å¯¹çƒ­ç‚¹ (Viral Hits) ---
    # é€»è¾‘ï¼šä¸åˆ†æ¿å—ï¼Œå…¨ç½‘ Score æœ€é«˜çš„å‰ 5 å
    viral_pool = sorted(posts, key=lambda x: x.get('score', 0), reverse=True)[:5]
    viral_ids = {p['url'] for p in viral_pool}  # è®°å½• ID ä»¥å…åç»­é‡å¤é€‰å…¥

    # --- ç­–ç•¥ 2: ğŸ“‰ å¸‚åœºä¸ç§‘æŠ€ä¿¡å· (Market Movers) ---
    # é€»è¾‘ï¼šåªçœ‹ç‰¹å®šé‡‘è/ç§‘æŠ€æ¿å—ï¼Œæ’é™¤å·²å…¥é€‰ Viral çš„
    market_pool = [
        p for p in posts 
        if p.get('subreddit') in TARGET_MARKET_SUBS 
        and p['url'] not in viral_ids
    ]
    market_pool.sort(key=lambda x: x.get('score', 0), reverse=True)
    market_top = market_pool[:5]  # å–å‰ 5

    # D. æ„å»º Markdown è¡¨æ ¼ (åˆ†å¼€å±•ç¤º)
    report_sections = {}

    def build_table(items, show_summary=False):
        if show_summary:
            # å¸‚åœºé£å‘ï¼šå¸¦æ‘˜è¦ï¼Œæ–¹ä¾¿çœ‹é€»è¾‘
            header = "| çƒ­åº¦ | r/æ¿å— | æ ‡é¢˜ & æ‘˜è¦ | æƒ…ç»ª | ğŸ”— |\n| :--- | :--- | :--- | :--- | :--- |"
        else:
            # å…¨çƒçƒ­æœï¼šåªçœ‹æ ‡é¢˜ï¼Œè¿½æ±‚ç®€æ´
            header = "| çƒ­åº¦ | r/æ¿å— | æ ‡é¢˜ | æƒ…ç»ª | ğŸ”— |\n| :--- | :--- | :--- | :--- | :--- |"
            
        rows = []
        for p in items:
            score = fmt_k(p.get('score', 0))
            sub = p.get('subreddit', 'unknown')
            title = p.get('title', '-').replace('|', '')[:50] + "..."
            
            # æƒ…ç»ª Emoji
            vibe_val = p.get('vibe', 0)
            if vibe_val > 0.2: vibe_icon = "ğŸ˜"
            elif vibe_val < -0.2: vibe_icon = "ğŸ˜¡"
            else: vibe_icon = "ğŸ˜"
            
            url = p.get('url', '#')
            
            if show_summary:
                # æ‘˜è¦å¤„ç†ï¼šè®©è¡¨æ ¼å†…å®¹æ›´ä¸°å¯Œ
                summary = p.get('summary', '').replace('\n', ' ')[:80] + "..."
                content_col = f"**{title}**<br>_{summary}_"
            else:
                content_col = f"**{title}**"

            rows.append(f"| {score} | `{sub}` | {content_col} | {vibe_icon} {vibe_val:.2f} | [ğŸ”—]({url}) |")
            
        return {"header": header, "rows": rows}

    # ç»„è£…æˆ˜æŠ¥ï¼šåˆ†ç±»ç‹¬ç«‹å±•ç¤º
    if viral_pool:
        report_sections["ğŸš¨ Reddit Viral (å…¨çƒçƒ­æœ)"] = build_table(viral_pool, show_summary=False)
    
    if market_top:
        report_sections["ğŸ“‰ Market & Tech (å¸‚åœºé£å‘)"] = build_table(market_top, show_summary=True)

    return report_sections
